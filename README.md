# Word Similarity API (Word2Vec + FastAPI)

![](images/result.gif)

## Overview

This project implements a domain-trained Word Similarity API using Word2Vec on large-scale technical text data (StackOverflow answers). The system learns semantic relationships between programming and computer science terms and exposes them via a lightweight FastAPI service.

Unlike generic pre-trained embeddings, this model is trained from scratch on technical discourse, enabling more meaningful similarities for software engineering, Python, and CS-related vocabulary.

## ğŸš€ Key Features

ğŸ”¹ Custom Word2Vec model trained on technical StackOverflow answers.

ğŸ”¹ Cleaned and normalized text pipeline designed for code-heavy content.

ğŸ”¹ REST API built with FastAPI for real-time word similarity queries.

ğŸ”¹ Explicit handling of out-of-vocabulary (OOV) words.

ğŸ”¹ Modular and extensible project structure.

## ğŸ—‚ï¸ Repository Structure

```bash
word-similarity-api/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api.py 
â”‚ â””â”€â”€ model.py 
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ stack_overflow_tech_final.parquet
â”‚
â”œâ”€â”€ images/
â”‚
â”œâ”€â”€ model/
â”‚ â””â”€â”€ word2vec.model
â”‚
â”œâ”€â”€ preprocessing/
â”‚ â”œâ”€â”€ data_cleaning.py
â”‚ â””â”€â”€ data_normalization.py
â”‚
â”œâ”€â”€ test.py 
â”œâ”€â”€ train.py 
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/your-username/word-similarity-api.git
cd Word_Similarity_API
```

### 2ï¸âƒ£ Create & Activate Virtual Environment

```
python -m venv venv
venv\Scripts\activate 
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r requirements.txt
```

### 4ï¸âƒ£ Train the Word2Vec Model

```
python train.py
```

### 5ï¸âƒ£ Run the API Server

```
uvicorn app.api:app --reload
```

Server will start at:

```
http://127.0.0.1:8000
```


## ğŸ—ï¸ Architecture Overview

```
Parquet Dataset
â†“
Text Cleaning & Normalization
â†“
Tokenized Sentences
â†“
Word2Vec Training (gensim)
â†“
Saved Model (.model)
â†“
FastAPI Inference Layer
```

## ğŸ—‚ï¸ Dataset

- **Source**: StackOverflow (technical Q&A content)

- **Field Used**: Accepted / high-quality answers

- **Format**: Parquet
  
- **Domain**: Python, programming, debugging, CS fundamentals

## ğŸ§  Why Word2Vec?

During experimentation, multiple embedding approaches were evaluated:

- **FastText**: Good subword coverage but produced noisy semantic results for technical terms

- **Word2Vec** (Skip-gram): Provided cleaner, concept-level semantic groupings

Given the goal of semantic clarity over morphological similarity, Word2Vec was selected as the final model.

## âš™ï¸ Training Pipeline

1. Load StackOverflow answers.

2. Clean & Normalize text (lowercasing, punctuation cleanup).

3. Tokenize into tokens.

4. Train Word2Vec embeddings.

5. Save trained model to disk.

## ğŸ”§ Train The Model

![](images/training.jpg)


## ğŸŒ API Usage

![](images/api.jpg)

### Endpoint

- GET `/similar-words`

### Query Parameters:

- `word` (string) : Input word

### Example Request

```
/similar-words?word=error
```

### Example Response

![](images/result_01.jpg)

## ğŸ›‘ Error Handling

- Empty input validation

- Model load failure handling

- Graceful response for OOV (out-of-vocabulary) words


## ğŸ” Testing The Model

The model was tested on different categories of words.

![](images/test_01.jpg)
![](images/test_02.jpg)
![](images/test_03.jpg)
![](images/test_04.jpg)
![](images/test_05.jpg)
![](images/test_06.jpg)

## ğŸ“Š Insights & Observations

### âœ… What the Model Does Well

1. Captures conceptual similarity
   - `iterator â†’ iterable, protocol`
   - `exception â†’ raise, catch, handling`
2. Understands StackOverflow semantics
   - `answer â†’ accepted, comment`
   - `question â†’ asked, answered` 
3. Distinguishes programming abstractions
   - `list â‰  array â‰  dictionary` 

### âš ï¸ Known Limitations

1. Weak performance on generic verbs (`get`, `make`, `use`)

2. No phrase awareness (`machine learning` treated as two words)

3. Purely distributional (no syntactic supervision)

## ğŸ›£ï¸ Future Improvements

- Phrase-aware embeddings with stricter thresholds

- Sentence-level embeddings (Sentence-BERT)

- Batch similarity endpoints

- Dockerized deployment

- Quantitative evaluation benchmarks

## ğŸ“Œ Final Note

This project prioritizes engineering judgment over chasing perfect scores. Every design decision is intentional, tested, and justified.
